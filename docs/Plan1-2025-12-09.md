# Context Task Management - Incremental Implementation Plan

## Philosophy: Feedback-Driven, Speed Over Perfection

**Core Principle:** Get something working ASAP, then iterate based on real usage.

**Approach:** Ollama endpoint → Test it works → Add UI → Add TickTick → Get feedback → Iterate

---

## Configuration Decisions

| Setting | Choice | Rationale |
|---------|--------|-----------|
| **Ollama Model** | `qwen3:4b` (fast) / `qwen3:8b` (accurate) | Already installed, good at tool calls |
| **TickTick** | Real credentials ready | Client ID + Secret available |
| **Settings Storage** | localStorage (frontend) | Simple, no backend persistence needed initially |
| **Database** | PostgreSQL via Docker | Easy to host later, production-ready |
| **Dev Environment** | Docker for DB, local for app | Fast iteration with proper DB |

---

## Iteration 0: Minimal Viable Demo (2-3 hours)

**Goal:** Prove Ollama (qwen3) can analyze a task description via FastAPI endpoint

**Demo:** POST request with task text → get back urgency/importance/quadrant scores

**Status:** ✅ **COMPLETED** (FastAPI `POST /api/analyze` + Ollama `qwen3` responding)

**Additional infrastructure completed:**
- ✅ PostgreSQL + Redis running via Docker (port 5433 to avoid local conflict)
- ✅ Database models created (User, Task, Settings) with SQLAlchemy
- ✅ Alembic migrations configured and applied
- ✅ Core configuration system with Pydantic Settings
- ✅ Database connection test scripts created

**Completed deliverables:**
- [x] FastAPI service running locally with `/health` and `/api/analyze`
- [x] Ollama reachable at `http://localhost:11434` with `qwen3` model
- [x] Quadrant calculation returned in JSON

**Quick verification:**
```bash
curl -s http://localhost:8000/health
curl -s -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"description": "Finish quarterly report by Friday"}'
```

### Files to Create

```
backend/
├── app/
│   ├── __init__.py
│   ├── main.py                    # FastAPI app with ONE endpoint
│   └── services/
│       ├── __init__.py
│       └── llm_ollama.py          # Ollama client
├── requirements.txt
└── .env.example
docker-compose.yml                  # PostgreSQL + Redis
```

### Dependencies (requirements.txt)
```
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
httpx==0.25.2
python-dotenv==1.0.0
sqlalchemy[asyncio]==2.0.23
asyncpg==0.29.0
alembic==1.13.0
```

### Docker Compose (docker-compose.yml)
```yaml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: context
      POSTGRES_PASSWORD: context_dev
      POSTGRES_DB: context
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
```

### Key Implementation Details

**Ollama Service (`llm_ollama.py`):**
- Connect to `http://localhost:11434`
- Default model: `qwen3:4b` (configurable to `qwen3:8b`)
- Use `/api/generate` endpoint with `format: "json"`
- Prompt returns: `{urgency: 1-10, importance: 1-10, reasoning: string}`
- Calculate quadrant: Q1 (U≥7, I≥7), Q2 (U<7, I≥7), Q3 (U≥7, I<7), Q4 (both <7)

**FastAPI Endpoint (`main.py`):**
- `POST /api/analyze` - Accepts task description, returns analysis
- `GET /health` - Health check
- CORS enabled for frontend

### What's Deferred
- Database (use in-memory)
- Authentication
- Background jobs
- Frontend
- TickTick integration

### Success Criteria
- FastAPI starts without errors
- Ollama responds within 5-10 seconds
- Quadrant assignment makes sense
- Can test via curl/Postman

---

## Iteration 1: Basic UI + Settings (3-4 hours)

**Goal:** Simple web page to test task analysis and configure LLM settings

**Demo:** Type a task in a form → see analysis instantly → configure Ollama URL

**Status:** ✅ **COMPLETED** (2025-12-10 - Fully tested and working)

**Completed Work (2025-12-10):**
- ✅ Backend infrastructure: PostgreSQL on port 5433, Redis, Alembic migrations
- ✅ Database models: User, Task, Settings with proper relationships
- ✅ API routers: Full CRUD for tasks + settings endpoints
- ✅ FastAPI integration: Routers added to main.py with CORS
- ✅ Next.js 14 frontend: TypeScript + Tailwind + shadcn/ui
- ✅ TaskAnalyzer component: AI-powered task analysis with quadrant display
- ✅ LLMSettings component: Redesigned with model selector (removed confusing URL field)
- ✅ Tabbed UI: Analyze Task | Settings (replaced static welcome page)
- ✅ API client: Backend integration with error handling

**Critical Bugs Fixed (2025-12-10):**
1. **Frontend/Backend field mismatch:**
   - Fixed TaskAnalyzer sending `task_description` instead of `description`
   - Fixed backend response fields: `urgency_score`, `importance_score`, `eisenhower_quadrant`
2. **Settings UI misleading architecture:**
   - Removed editable "Ollama Provider URL" field (was unused)
   - Made Backend URL read-only: `http://127.0.0.1:8000`
   - Added model selector dropdown (qwen3:4b / qwen3:8b)
   - Added "Architecture Overview" card explaining request flow
   - Fixed connection status check (`healthy` → `ok`)

**Testing Status:**
- ✅ Backend endpoints: `/health`, `/api/analyze`, `/api/llm/health`, `/api/llm/models` verified
- ✅ Frontend build: Successful compilation, no errors
- ✅ End-to-end testing: **COMPLETE** via Playwright
- ✅ Task analysis: Working with Ollama qwen3:4b (Q1, Q2, Q4 quadrants tested)
- ✅ Settings tab: Connection testing, model selection, localStorage persistence working
- ✅ No console errors, all API calls returning 200 OK

**Architecture Validated:**
- Frontend (localhost:3000) → Backend (127.0.0.1:8000) → Ollama (127.0.0.1:11434)
- Frontend NEVER communicates directly with Ollama (security/flexibility)
- All LLM requests proxied through backend FastAPI service

**Ready for Phase 2: TickTick OAuth Integration**

**Current Todo List (2025-12-10):**
- ✅ Phase 1: Replace page.tsx with tabbed interface (Analyze | Settings)
- ✅ Phase 1: Test tabbed UI and manual task analysis
- ✅ Phase 1: Fix Settings UI - remove URL field, add model selector
- ⏭️ Phase 2: Create TickTick service (backend/app/services/ticktick.py)
- ⏭️ Phase 2: Create auth router (backend/app/api/auth.py)
- ⏭️ Phase 2: Register auth router in main.py
- ⏭️ Phase 2: Add sync endpoint to tasks.py
- ⏭️ Phase 3: Create OAuth callback page (frontend/app/auth/callback/page.tsx)
- ⏭️ Phase 3: Add TickTick connection UI to Settings tab
- ⏭️ Phase 3: Add sync UI to Analyze tab
- ⏭️ Phase 4: End-to-end testing with Playwright

**Validation Complete:**
- ✅ `npm install && npm run dev` in `frontend/` loads `http://localhost:3000`
- ✅ Analyze form returns urgency/importance/quadrant for sample tasks
- ✅ Settings model selection persists in localStorage
- ✅ Clear error messages when Ollama/backend unreachable
- ✅ Backend URL standardized to `http://127.0.0.1:8000`
- ✅ Zero console errors during tab navigation and API calls

### Files to Create

```
frontend/
├── package.json
├── next.config.js
├── tsconfig.json
├── tailwind.config.ts
├── postcss.config.js
├── src/
│   ├── app/
│   │   ├── layout.tsx
│   │   ├── page.tsx              # Main page with tabs
│   │   ├── globals.css
│   │   └── settings/
│   │       └── page.tsx          # Settings page
│   ├── components/
│   │   ├── TaskAnalyzer.tsx      # Form + results display
│   │   └── LLMSettings.tsx       # Provider configuration
│   └── lib/
│       └── api.ts                # Backend API client
```

### Frontend Stack
- Next.js 14 (App Router)
- TypeScript
- Tailwind CSS
- shadcn/ui components (Button, Input, Card, Select)

### Key Components

**TaskAnalyzer.tsx:**
- Textarea for task description
- "Analyze" button
- Results display: urgency, importance, quadrant, reasoning
- Loading state

**LLMSettings.tsx:** (Redesigned 2025-12-10)
- Read-only Backend URL display (`http://127.0.0.1:8000`)
- Model selector dropdown (qwen3:4b, qwen3:8b)
- "Test Connection" button (tests backend → Ollama connectivity)
- "Save Settings" button (persists model choice to localStorage)
- Connection status card (green/red/blue with detailed messages)
- Architecture Overview card (explains Frontend → Backend → Ollama flow)
- Note: Removed editable "Ollama URL" field to prevent confusion (backend handles this)

### Backend Addition
- `GET /api/llm/health` - Check if Ollama is reachable
- `GET /api/llm/models` - List available Ollama models

### What's Deferred
- Task persistence
- User authentication
- TickTick integration

### Success Criteria - ✅ ALL MET (2025-12-10)
- ✅ Can type task and see analysis (Q1, Q2, Q4 quadrants verified)
- ✅ Model selection persists in localStorage across browser refresh
- ✅ Clear error messages when Ollama/backend unavailable
- ✅ Tabbed interface working (Analyze | Settings)
- ✅ Backend URL clearly displayed (`http://127.0.0.1:8000`)
- ✅ Architecture flow clearly explained to users
- ✅ Zero console errors during normal operation
- ✅ All API endpoints returning proper responses (200 OK)

**Files Modified (2025-12-10):**
- `frontend/app/page.tsx` - Replaced static welcome with tabbed interface
- `frontend/components/TaskAnalyzer.tsx` - Fixed API field name (`description`)
- `frontend/components/LLMSettings.tsx` - Complete redesign (removed URL field, added model selector)
- `backend/app/main.py` - Fixed response field names (`urgency_score`, `importance_score`, `eisenhower_quadrant`)
- `frontend/components/ui/alert.tsx` - Added via shadcn CLI

---

## Iteration 2: TickTick Integration (4-5 hours)

**Goal:** Fetch real tasks from TickTick, analyze them, display in a list

**Demo:** Click "Sync" → see actual TickTick tasks with quadrant assignments

### Backend Changes

**New Files:**
```
backend/app/
├── core/
│   ├── __init__.py
│   ├── config.py               # Environment variables (pydantic-settings)
│   └── database.py             # PostgreSQL async setup
├── models/
│   ├── __init__.py
│   └── task.py                 # Task model
├── services/
│   └── ticktick.py             # TickTick API client
└── api/
    ├── __init__.py
    ├── tasks.py                # Task CRUD endpoints
    └── auth.py                 # TickTick OAuth endpoints
```

**Database:** PostgreSQL via Docker (already running from Iteration 0)

**Task Model:**
- id, ticktick_id, title, description
- urgency, importance, quadrant, reasoning
- created_at, updated_at

**TickTick Service:**
- Full OAuth flow (you have Client ID + Secret)
- Authorization URL → Callback → Token exchange
- `GET /task` - Fetch all tasks
- Map to internal Task model

**New Endpoints:**
- `GET /api/auth/ticktick` - Redirect to TickTick OAuth
- `GET /api/auth/ticktick/callback` - Handle OAuth callback
- `POST /api/tasks/sync` - Fetch from TickTick, analyze, store
- `GET /api/tasks` - List all analyzed tasks
- `GET /api/tasks/{id}` - Get single task

### Frontend Changes

**New Component: TaskList.tsx**
- "Sync TickTick" button
- List of tasks with quadrant badges
- Urgency/importance scores
- Reasoning preview

**Update page.tsx:**
- Tab navigation: Analyze | My Tasks | Settings

### What's Deferred
- Bi-directional sync (read-only for now)
- Webhooks (polling first)
- Background jobs (Celery)
- Matrix visualization (list view first)

### Success Criteria
- Real TickTick tasks sync successfully
- Analysis completes within 30s for 10 tasks
- Tasks persist in SQLite
- Can refresh and see synced tasks

---

## Iteration 3: Multiple LLM Providers + Polish (3-4 hours)

**Goal:** Support Ollama, Gemini, OpenRouter with provider selection UI

**Demo:** Switch between providers, compare accuracy and speed

### Backend Changes

**New Files:**
```
backend/app/services/
├── llm/
│   ├── __init__.py
│   ├── base.py                 # Abstract base class
│   ├── factory.py              # Provider factory
│   ├── ollama.py               # Ollama (moved)
│   ├── gemini.py               # Google Gemini
│   └── openrouter.py           # OpenRouter (DeepSeek, etc.)
```

**Provider Interface (base.py):**
```python
class LLMProvider(ABC):
    async def analyze_task(self, description: str) -> TaskAnalysis
    async def health_check(self) -> bool
```

**Factory (factory.py):**
- Get provider by name
- Health check before use
- Fallback logic (optional)

**New Dependencies:**
```
google-generativeai==0.3.2
```

**Environment Variables:**
```
LLM_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
GEMINI_API_KEY=...
OPENROUTER_API_KEY=...
OPENROUTER_MODEL=deepseek/deepseek-chat
```

### Frontend Changes

**Enhanced LLMSettings.tsx:**
- Provider selector: Ollama | Gemini | OpenRouter
- Provider-specific config fields
- Model dropdown per provider
- Connection status indicator
- Save to backend (new endpoint)

**New Endpoint:**
- `GET /api/settings` - Get current LLM config
- `PUT /api/settings` - Update LLM config
- Store in SQLite settings table

### What's Deferred
- Provider-specific prompts
- Cost tracking
- Rate limiting
- User accounts

### Success Criteria
- All 3 providers work
- Can switch without restart
- Settings persist in database
- Clear status indicators

---

## File Creation Order

### Iteration 0 (Backend MVP)
1. `backend/requirements.txt`
2. `backend/.env.example`
3. `backend/app/__init__.py`
4. `backend/app/main.py`
5. `backend/app/services/__init__.py`
6. `backend/app/services/llm_ollama.py`

### Iteration 1 (Frontend MVP)
7. `frontend/package.json`
8. `frontend/next.config.js`
9. `frontend/tsconfig.json`
10. `frontend/tailwind.config.ts`
11. `frontend/postcss.config.js`
12. `frontend/src/app/globals.css`
13. `frontend/src/app/layout.tsx`
14. `frontend/src/app/page.tsx`
15. `frontend/src/lib/api.ts`
16. `frontend/src/components/TaskAnalyzer.tsx`
17. `frontend/src/components/LLMSettings.tsx`
18. `frontend/src/app/settings/page.tsx`

### Iteration 2 (TickTick)
19. `backend/app/core/__init__.py`
20. `backend/app/core/config.py`
21. `backend/app/core/database.py`
22. `backend/app/models/__init__.py`
23. `backend/app/models/task.py`
24. `backend/app/services/ticktick.py`
25. `backend/app/api/__init__.py`
26. `backend/app/api/tasks.py`
27. `frontend/src/components/TaskList.tsx`

### Iteration 3 (Multi-Provider)
28. `backend/app/services/llm/__init__.py`
29. `backend/app/services/llm/base.py`
30. `backend/app/services/llm/factory.py`
31. `backend/app/services/llm/ollama.py` (refactored)
32. `backend/app/services/llm/gemini.py`
33. `backend/app/services/llm/openrouter.py`
34. `backend/app/models/settings.py`
35. `backend/app/api/settings.py`

---

## Testing Strategy

### Iteration 0
```bash
# Terminal 1: Start Docker services
docker-compose up -d

# Terminal 2: Ensure Ollama is running with qwen3
ollama run qwen3:4b

# Terminal 3: Start backend
cd backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000

# Test
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"description": "Finish quarterly report by Friday"}'
```

### Iteration 1
```bash
# Terminal 4: Start frontend
cd frontend
npm install
npm run dev

# Visit http://localhost:3000
# Type task, click Analyze
# Go to Settings, verify Ollama connection
```

### Iteration 2
```bash
# Add TickTick credentials to .env
TICKTICK_CLIENT_ID=your_client_id
TICKTICK_CLIENT_SECRET=your_client_secret

# In UI: Click "Connect TickTick" → OAuth flow
# Click "Sync" → Verify tasks appear with analysis
```

---

## Decision Points

After each iteration, evaluate:

1. **Accuracy:** Do quadrant assignments match intuition?
2. **Speed:** Is analysis fast enough (<5s for single task)?
3. **Usability:** Would you use this daily?

**If NO to any:**
- Iteration 0: Try `qwen3:8b` for better accuracy, or add Gemini
- Iteration 1: Simplify UI further
- Iteration 2: Check if TickTick integration adds value
